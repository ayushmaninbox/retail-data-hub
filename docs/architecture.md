# ğŸ—ï¸ Architecture â€” Retail Data Hub

## 1. System Overview

The Retail Data Hub follows the **Medallion Architecture** (Bronze â†’ Silver â†’ Gold) pattern,
providing a scalable and maintainable data pipeline that transforms raw, siloed retail data
into analytics-ready star schema tables.

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                              DATA SOURCES                                       â”‚
â”‚                                                                                  â”‚
â”‚       50+ POS Stores          E-Commerce Portal         Warehouse WMS           â”‚
â”‚       (CSV, daily batch)      (JSON, near real-time)    (CSV, daily batch)      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚                              â”‚                        â”‚
         â–¼                              â–¼                        â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  ğŸ“¥ INGESTION LAYER (src/ingestion/)                                            â”‚
â”‚                                                                                  â”‚
â”‚  ingest_batch.py          ingest_realtime.py          schema_validator.py        â”‚
â”‚  â”œâ”€ CSV reader            â”œâ”€ JSON reader              â”œâ”€ Schema registry        â”‚
â”‚  â”œâ”€ Schema validation     â”œâ”€ Schema validation        â”œâ”€ Type coercion          â”‚
â”‚  â”œâ”€ Retry w/ backoff      â”œâ”€ Retry w/ backoff         â”œâ”€ Missing col fill       â”‚
â”‚  â””â”€ Write Parquet         â””â”€ Write Parquet            â””â”€ Ingestion logging      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                   â”‚
                                   â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  ğŸ¥‰ BRONZE LAYER (data/bronze/)                                                 â”‚
â”‚                                                                                  â”‚
â”‚  Append-only, schema-validated Parquet files                                    â”‚
â”‚  â”œâ”€â”€ pos_sales/pos_sales.parquet         (~50,000 rows, 11 columns)             â”‚
â”‚  â”œâ”€â”€ web_orders/web_orders.parquet       (~15,000 rows, 13 columns)             â”‚
â”‚  â”œâ”€â”€ warehouse/warehouse_inventory.parquet (~50,000 rows, 9 columns)            â”‚
â”‚  â””â”€â”€ warehouse/shipments.parquet         (~8,000 rows, 8 columns)               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â”‚                      â”‚
                  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”˜                      â”‚
                  â–¼                               â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  âœ… DATA QUALITY CHECKS         â”‚  â”‚  ğŸ”„ TRANSFORMATION LAYER                   â”‚
â”‚  (src/quality/)                 â”‚  â”‚  (src/transformation/)                     â”‚
â”‚                                 â”‚  â”‚                                            â”‚
â”‚  7 automated rules:             â”‚  â”‚  bronze_to_silver.py                       â”‚
â”‚  1. No negative prices          â”‚  â”‚  â”œâ”€ Dedup on composite keys               â”‚
â”‚  2. No future dates             â”‚  â”‚  â”œâ”€ Null imputation                        â”‚
â”‚  3. No null customer IDs        â”‚  â”‚  â”œâ”€ Business logic validation              â”‚
â”‚  4. No duplicate rows           â”‚  â”‚  â”œâ”€ Channel unification (POS + Web)        â”‚
â”‚  5. Referential integrity       â”‚  â”‚  â””â”€ Write to Silver Parquet                â”‚
â”‚  6. Quantity range checks       â”‚  â”‚                                            â”‚
â”‚  7. Column completeness         â”‚  â”‚  silver_to_gold.py                         â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€     â”‚  â”‚  â”œâ”€ SCD Type 2 (customer dim)              â”‚
â”‚  Output:                        â”‚  â”‚  â”œâ”€ Surrogate key generation               â”‚
â”‚  data_quality_report.json       â”‚  â”‚  â”œâ”€ Star schema assembly                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚  â””â”€ Date-partitioned Parquet output        â”‚
                                     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                                       â”‚
                                                       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  ğŸ¥ˆ SILVER LAYER (data/silver/)                                                 â”‚
â”‚                                                                                  â”‚
â”‚  Cleaned, deduplicated, type-corrected data                                     â”‚
â”‚  â”œâ”€â”€ unified_sales.parquet       (POS + Web merged, clean)                      â”‚
â”‚  â”œâ”€â”€ inventory_clean.parquet     (duplicates removed, validated)                â”‚
â”‚  â””â”€â”€ shipments_clean.parquet     (orphans rejected, dates fixed)                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                           â”‚  star schema + SCD Type 2
                                           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  ğŸ¥‡ GOLD LAYER (data/gold/)  â€”  Star Schema                                    â”‚
â”‚                                                                                  â”‚
â”‚  DIMENSION TABLES                    FACT TABLES                                â”‚
â”‚  â”œâ”€â”€ dim_date.parquet                â”œâ”€â”€ fact_sales.parquet                      â”‚
â”‚  â”œâ”€â”€ dim_customer.parquet (SCD2)     â”œâ”€â”€ fact_inventory.parquet                  â”‚
â”‚  â”œâ”€â”€ dim_product.parquet             â””â”€â”€ fact_shipments.parquet                  â”‚
â”‚  â””â”€â”€ dim_store.parquet                                                          â”‚
â”‚                                                                                  â”‚
â”‚  Partitioned by: date (monthly), region (city)                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                               â”‚
              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
              â–¼                â–¼                â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ ğŸ“Š DASHBOARD      â”‚ â”‚ ğŸ“ SQL QUERIES â”‚ â”‚ ğŸ“ˆ ANALYTICS ENGINE  â”‚
â”‚ (Streamlit +      â”‚ â”‚ (sql/)         â”‚ â”‚ (src/analytics/)     â”‚
â”‚  Plotly)          â”‚ â”‚                â”‚ â”‚                      â”‚
â”‚ 7 interactive     â”‚ â”‚ Pure SQL KPIs  â”‚ â”‚ KPI computation      â”‚
â”‚ tabs covering     â”‚ â”‚ running on     â”‚ â”‚ Market Basket (ML)   â”‚
â”‚ all KPI           â”‚ â”‚ DuckDB OLAP   â”‚ â”‚ RFM segmentation     â”‚
â”‚ categories        â”‚ â”‚ engine         â”‚ â”‚ CLV analysis         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## 2. Data Flow Diagram (Mermaid)

```mermaid
flowchart TD
    subgraph Sources["ğŸ“¡ Data Sources"]
        POS["ğŸª 50+ POS Stores<br/>CSV â€¢ Daily Batch"]
        WEB["ğŸŒ E-Commerce<br/>JSON â€¢ Near Real-Time"]
        WH["ğŸ“¦ Warehouse WMS<br/>CSV â€¢ Daily Batch"]
    end

    subgraph Ingestion["ğŸ“¥ Ingestion Layer"]
        IB["ingest_batch.py<br/>CSV â†’ Parquet"]
        IR["ingest_realtime.py<br/>JSON â†’ Parquet"]
        SV["schema_validator.py<br/>Schema Registry + Retry"]
    end

    subgraph Bronze["ğŸ¥‰ Bronze Layer"]
        B_POS["pos_sales.parquet<br/>~50K rows"]
        B_WEB["web_orders.parquet<br/>~15K rows"]
        B_INV["warehouse_inventory.parquet<br/>~50K rows"]
        B_SHP["shipments.parquet<br/>~8K rows"]
    end

    subgraph Quality["âœ… Data Quality"]
        DQ["7 Automated Checks<br/>quality_checks.py"]
        DQR["data_quality_report.json"]
    end

    subgraph Transform["ğŸ”„ Transformation"]
        B2S["bronze_to_silver.py<br/>Clean â€¢ Dedup â€¢ Merge"]
        S2G["silver_to_gold.py<br/>Star Schema â€¢ SCD2"]
    end

    subgraph Silver["ğŸ¥ˆ Silver Layer"]
        S_SALES["unified_sales.parquet"]
        S_INV["inventory_clean.parquet"]
        S_SHP["shipments_clean.parquet"]
    end

    subgraph Gold["ğŸ¥‡ Gold Layer â€” Star Schema"]
        DIM_D["dim_date"]
        DIM_C["dim_customer<br/>(SCD Type 2)"]
        DIM_P["dim_product"]
        DIM_S["dim_store"]
        F_SALES["fact_sales"]
        F_INV["fact_inventory"]
        F_SHP["fact_shipments"]
    end

    subgraph Analytics["ğŸ“Š Analytics & Presentation"]
        DASH["Streamlit Dashboard<br/>7 Interactive Tabs"]
        SQL["SQL Queries<br/>DuckDB OLAP"]
        KPI["KPI Engine<br/>Python Analytics"]
        ML["Market Basket<br/>Apriori Algorithm"]
    end

    POS --> IB
    WH --> IB
    WEB --> IR
    IB --> SV
    IR --> SV
    SV --> B_POS
    SV --> B_WEB
    SV --> B_INV
    SV --> B_SHP

    B_POS --> DQ
    B_WEB --> DQ
    B_INV --> DQ
    B_SHP --> DQ
    DQ --> DQR

    B_POS --> B2S
    B_WEB --> B2S
    B_INV --> B2S
    B_SHP --> B2S
    B2S --> S_SALES
    B2S --> S_INV
    B2S --> S_SHP

    S_SALES --> S2G
    S_INV --> S2G
    S_SHP --> S2G
    S2G --> DIM_D
    S2G --> DIM_C
    S2G --> DIM_P
    S2G --> DIM_S
    S2G --> F_SALES
    S2G --> F_INV
    S2G --> F_SHP

    F_SALES --> DASH
    F_SALES --> SQL
    F_SALES --> KPI
    F_SALES --> ML
    F_INV --> DASH
    F_SHP --> DASH
```

---

## 3. Star Schema ERD

```mermaid
erDiagram
    dim_date {
        int date_key PK
        date full_date
        int year
        int quarter
        int month
        string month_name
        int day
        string day_name
        boolean is_weekend
        string season
    }

    dim_customer {
        int customer_key PK
        string customer_id
        string customer_name
        string city
        string state
        date effective_from
        date effective_to
        boolean is_current
        int scd_version
    }

    dim_product {
        int product_key PK
        string product_id
        string product_name
        string category
        float base_price
    }

    dim_store {
        int store_key PK
        string store_id
        string store_name
        string city
        string state
        string store_format
    }

    fact_sales {
        int sale_key PK
        int date_key FK
        int customer_key FK
        int product_key FK
        int store_key FK
        string channel
        string transaction_id
        int quantity
        float unit_price
        float total_amount
        string payment_method
    }

    fact_inventory {
        int inventory_key PK
        int date_key FK
        int product_key FK
        int store_key FK
        int quantity_on_hand
        int reorder_level
        float unit_cost
        boolean is_stockout
    }

    fact_shipments {
        int shipment_key PK
        int ship_date_key FK
        int delivery_date_key FK
        int store_key FK
        string order_id
        string destination_city
        int transit_days
        string status
        string carrier
    }

    dim_date ||--o{ fact_sales : "date_key"
    dim_customer ||--o{ fact_sales : "customer_key"
    dim_product ||--o{ fact_sales : "product_key"
    dim_store ||--o{ fact_sales : "store_key"
    dim_date ||--o{ fact_inventory : "date_key"
    dim_product ||--o{ fact_inventory : "product_key"
    dim_store ||--o{ fact_inventory : "store_key"
    dim_date ||--o{ fact_shipments : "ship_date_key"
    dim_store ||--o{ fact_shipments : "store_key"
```

---

## 4. Design Decisions & Trade-offs

### Why Medallion Architecture?

| Decision | Rationale |
|---|---|
| **Bronze = raw Parquet** | Preserves source fidelity; schema validation at write time catches format drift early |
| **Silver = cleaned** | Single layer for dedup, null handling, type coercion â€” avoids over-engineering |
| **Gold = star schema** | Optimized for analytical queries; DuckDB and BI tools work best with star models |

### Why DuckDB over Spark / BigQuery?

| Factor | DuckDB | Spark / Cloud |
|---|---|---|
| Setup | `pip install duckdb` | Cluster provisioning, cloud accounts |
| Cost | $0 | $$$$ |
| Performance (at our scale) | ~50ms per query on 50K rows | Overkill |
| Portability | Runs anywhere Python does | Cloud-dependent |
| Hackathon fit | âœ… Perfect | âŒ Over-engineered |

### Why Parquet?

- **Columnar** â€” only reads needed columns in analytical queries
- **Compressed** â€” 5-10Ã— smaller than CSV, lower disk I/O
- **Schema enforcement** â€” types are preserved on disk, no CSV parsing ambiguity
- **Ecosystem** â€” native support in Pandas, DuckDB, Spark, Polars

### Schema Evolution Strategy

The `schema_validator.py` module handles format changes:
- **Missing columns** â†’ filled with safe defaults (`"UNKNOWN"`, `0`, `NaT`)
- **Extra columns** â†’ kept, logged as warning (backward-compatible)
- **Type drift** â†’ coerced with fallbacks (e.g., `pd.to_numeric(errors="coerce")`)
- **strict mode** â†’ can raise on missing columns for critical pipelines

### Retry & Resilience

All ingestion functions use the `@retry_with_backoff` decorator:
- 3 attempts max
- Exponential backoff (1s â†’ 2s â†’ 4s)
- Catches transient I/O failures (network mounts, locked files)

---

## 5. Data Volume Summary

| Dataset | Rows | Columns | Format | Source |
|---|---|---|---|---|
| POS Sales | ~50,000 | 11 | CSV â†’ Parquet | 50+ stores, 2 years, 160+ products |
| Web Orders | ~15,000 | 13 | JSON â†’ Parquet | E-commerce, UPI/CC/COD payments |
| Warehouse Inventory | ~50,000 | 9 | CSV â†’ Parquet | Monthly snapshots, 50+ stores |
| Shipments | ~8,000 | 8 | CSV â†’ Parquet | 7 carriers, delivery tracking |

**Total:** ~123,000 records across 4 datasets, covering Jan 2023 â€“ Jan 2025.
