<p align="center">
  <h1 align="center">ğŸ§  Retail Data Hub</h1>
  <p align="center">
    <strong>Smart Retail Supply Chain & Customer Intelligence Platform</strong>
  </p>
  <p align="center">
    A full-stack data engineering project demonstrating ETL pipelines, Medallion Architecture, star schema modeling, automated data quality, KPI analytics, a FastAPI backend, and an interactive Next.js dashboard â€” all at zero infrastructure cost.
  </p>
</p>

---

## âœ¨ Highlights

| Capability | Implementation |
|---|---|
| **Architecture** | Medallion (Raw â†’ Bronze â†’ Silver â†’ Gold) with Parquet storage |
| **Data Modeling** | Star schema with fact & dimension tables, SCD Type 2 |
| **Ingestion** | Batch (CSV) + near real-time (JSON) with schema validation & retry |
| **Data Quality** | 7 automated checks with JSON evidence reports |
| **Analytics** | Commercial, Operations, Customer KPIs + Market Basket (Apriori) |
| **API** | FastAPI backend serving Gold layer KPIs with Swagger docs |
| **Dashboard** | 7-page interactive Next.js app with Recharts, Tailwind CSS |
| **Query Engine** | DuckDB â€” in-process SQL directly on Parquet files |
| **Cost** | $0 â€” Python + DuckDB + Next.js |

---

## ğŸ—ï¸ Architecture

![Retail Data Hub Architecture](/Users/ayushmaninbox/.gemini/antigravity/brain/ec357741-fe32-45ad-9dd7-17720c70ff90/retail_data_hub_architecture_v2_1770974777549.png)

---

## ğŸ“‚ Project Structure

```
retail-data-hub/
â”‚
â”œâ”€â”€ data/                              # All data layers (ignored by git)
â”‚   â”œâ”€â”€ raw/                           #   Source files (CSV, JSON)
â”‚   â”œâ”€â”€ bronze/                        #   Ingested Parquet (schema-validated)
â”‚   â”œâ”€â”€ silver/                        #   Cleaned & merged Parquet
â”‚   â”œâ”€â”€ gold/                          #   Star schema Parquet (fact/dims)
â”‚   â”œâ”€â”€ analytics/                     #   KPI and ML Forecast output (JSON)
â”‚   â””â”€â”€ logs/                          #   Pipeline execution logs
â”‚
â”œâ”€â”€ src/                               # All core logic
â”‚   â”œâ”€â”€ data_generation/               #   Realistic synthetic generators
â”‚   â”œâ”€â”€ ingestion/                     #   Raw â†’ Bronze layer
â”‚   â”œâ”€â”€ transformation/                #   Bronze â†’ Silver â†’ Gold
â”‚   â”œâ”€â”€ quality/                       #   Data Quality framework
â”‚   â”œâ”€â”€ analytics/                     #   KPI computation scripts
â”‚   â”œâ”€â”€ ml/                            #   ğŸ§  LSTM Demand Forecasting (PyTorch)
â”‚   â””â”€â”€ api/                           #   FastAPI backend
â”‚
â”œâ”€â”€ dashboard/                         # Next.js 14 Dashboard
â”‚   â”œâ”€â”€ src/app/forecast/              #   ğŸ§  AI Forecast page
â”‚   â””â”€â”€ ...                            #   Other analytics pages
â”‚
â”œâ”€â”€ scripts/                           # One-click automation
â”‚   â”œâ”€â”€ cleanup.sh                     #   ğŸ§¹ Reset demo state
â”‚   â”œâ”€â”€ forecast.sh                    #   ğŸ§  Run LSTM training
â”‚   â”œâ”€â”€ generation.sh                  #   Generate raw data
â”‚   â”œâ”€â”€ ingestion.sh                   #   Load Bronze parquets
â”‚   â”œâ”€â”€ transform.sh                   #   Build Gold star schema
â”‚   â”œâ”€â”€ kpi_analysis.sh                #   Compute JSON analytics
â”‚   â”œâ”€â”€ quality_checks.sh              #   Verify data firewall
â”‚   â”œâ”€â”€ api.sh                         #   Launch backend
â”‚   â””â”€â”€ dashboard.sh                   #   Launch frontend
```

---

## ğŸš€ Quick Start

### Prerequisites

- **Python 3.9+**
- **Node.js 18+** & npm
- **Git**

### 1. Clone & Install (one-time setup)

```bash
git clone https://github.com/ayushmaninbox/retail-data-hub.git
cd retail-data-hub
```

**macOS / Linux:**
```bash
chmod +x scripts/*.sh
./scripts/installation.sh
```

**Windows (PowerShell / CMD):**
```powershell
# Create venv and install dependencies
python -m venv .venv
.venv\Scripts\activate
python -m pip install --upgrade pip
pip install -r requirements.txt

# Install Dashboard dependencies
cd dashboard
npm install
cd ..

# Create data directories
mkdir data\raw, data\bronze, data\silver, data\gold, data\analytics
```

---

## ğŸ“Š Running the Platform

### ğŸ§¹ Step 0: The "Clean Slate" (Reset Button)
Run this once right before you start your demo to impress the judges by showing how the system builds from scratch.

**macOS / Linux:** `./scripts/cleanup.sh`  
**Windows:** `rm -Recurse -Force data/silver/*, data/gold/*, data/analytics/*` (or similar)

---

### ğŸ§ª Step 1: Terminal 1 â€” The Data Pipeline
Run these one by one to build the "Retail Brain".

| Step | macOS / Linux Command | Windows (Activated Venv) |
|---|---|---|
| **1. Generate Data** | `./scripts/generation.sh` | `python src/data_generation/generate_pos.py` |
| **2. Ingest** | `./scripts/ingestion.sh` | `python src/ingestion/ingest_batch.py` |
| **3. Transform** | `./scripts/transform.sh` | `python src/transformation/bronze_to_silver.py` |
| **4. Data Quality** | `./scripts/quality_checks.sh` | `python src/quality/quality_checks.py` |
| **5. KPI Analysis** | `./scripts/kpi_analysis.sh` | `python src/analytics/commercial_kpis.py` |
| **6. AI Forecast** | `./scripts/forecast.sh` | `python src/ml/demand_forecast.py` |

---

### ğŸš€ Step 2: Terminal 2 â€” Start API Server
Start this once the pipeline data is ready.

**macOS / Linux:** `./scripts/api.sh`  
**Windows:** `python src/api/api.py`

> ğŸŸ¢ FastAPI runs on **http://localhost:8000** | ğŸ“š Docs: **http://localhost:8000/docs**

---

### ğŸ–¥ï¸ Step 3: Terminal 3 â€” Start Dashboard
Keep this running to show the final visualization.

**macOS / Linux:** `./scripts/dashboard.sh`  
**Windows:** `cd dashboard && npm run dev`

> ğŸŸ¢ Next.js dashboard on **http://localhost:3000**

---

## ğŸ–¥ï¸ Dashboard Pages

The dashboard is a **Next.js 14** app built with **TypeScript**, **Tailwind CSS**, and **Recharts**. It fetches live data from the FastAPI backend.

| # | Page | Route | What You'll See |
|---|---|---|---|
| 1 | ğŸ  **Overview** | `/` | Revenue, orders, customers, avg transaction KPI cards + revenue trend |
| 2 | ğŸ“Š **Sales Analytics** | `/sales` | City-wise sales, top products, channel mix, monthly trends |
| 3 | ğŸ“¦ **Inventory Health** | `/inventory` | Stockout alerts, turnover ratio, reorder recommendations |
| 4 | ğŸšš **Logistics** | `/logistics` | Avg delivery time, delay distribution, seasonal demand |
| 5 | ğŸ‘¥ **Customers** | `/customers` | New vs returning, CLV distribution, RFM segments |
| 6 | ğŸ›’ **Market Basket** | `/market-basket` | Item associations, confidence scores, recommendation pairs |
| 7 | âœ… **Data Quality** | `/data-quality` | Pipeline health, row counts, check pass/fail status |

---

## ğŸš€ API Endpoints

The FastAPI backend serves pre-computed KPI data from the Gold layer as JSON:

| Method | Endpoint | Description |
|---|---|---|
| `GET` | `/api/health` | Health check + data file status |
| `GET` | `/api/overview` | Combined summary for the Overview page |
| `GET` | `/api/commercial` | Revenue, city sales, top products, channel mix |
| `GET` | `/api/operations` | Inventory turnover, stockout rate, delivery times |
| `GET` | `/api/customers` | CLV, RFM segmentation, new vs returning |
| `GET` | `/api/market-basket` | Market Basket Analysis (Apriori) results |
| `GET` | `/api/data-quality` | Data quality report |
| `GET` | `/api/sales` | Sales-specific data (derived from commercial) |
| `GET` | `/api/inventory` | Inventory-specific data (derived from operations) |
| `GET` | `/api/logistics` | Logistics-specific data (derived from operations) |

---

## ğŸ”‘ Key KPIs

### ğŸ“ˆ Commercial
- Daily / monthly revenue aggregation
- City-wise sales breakdown (10 Indian cities)
- Top 10 products by quantity sold
- Channel mix â€” POS vs Web revenue split
- Category-level revenue analysis

### ğŸ“¦ Operations
- Inventory turnover ratio per product/store
- Average delivery time per route
- Stockout rate (% of products with zero stock)
- Seasonal demand trends (monthly quantity by category)
- Reorder point alerts

### ğŸ‘¥ Customer
- New vs returning customer counts
- Customer Lifetime Value (CLV)
- RFM segmentation (Recency Â· Frequency Â· Monetary)

### ğŸ›’ AI / ML
- Market basket analysis using **Apriori algorithm**
- Standard, cross-channel, and category-level association rules
- Support, confidence & lift scores

---

## ğŸ›¡ï¸ Data Quality Framework

Seven automated checks run at every pipeline stage:

| # | Check | Rule | On Failure |
|---|---|---|---|
| 1 | No negative prices | `unit_price >= 0` | Quarantine row |
| 2 | No future dates | `date <= today()` | Reject row |
| 3 | No null customer IDs | `customer_id IS NOT NULL` | Fill "UNKNOWN" |
| 4 | No duplicates | Composite key uniqueness | Drop duplicate |
| 5 | Referential integrity | FK exists in dimension | Reject orphan |
| 6 | Quantity range | `1 <= qty <= 10,000` | Flag outlier |
| 7 | Column completeness | `% non-null per column` | Report metric |

Results are saved as `data/data_quality_report.json` and visualized in the Data Quality dashboard page.

---

## ğŸ§° Tech Stack

| Layer | Technology |
|---|---|
| **Language** | Python 3.9+ Â· TypeScript |
| **Data Generation** | Faker, Pandas, NumPy |
| **Storage Format** | Apache Parquet (columnar, compressed) |
| **Query Engine** | DuckDB (in-process OLAP) |
| **Transformations** | Pandas, DuckDB SQL |
| **ML / Analytics** | mlxtend (Apriori), scikit-learn, Pandas |
| **API Backend** | FastAPI + Uvicorn |
| **Dashboard** | Next.js 14 + React 18 + TypeScript |
| **Charts** | Recharts |
| **Styling** | Tailwind CSS |
| **UI Components** | Lucide React (icons) |
| **Automation** | Shell scripts (Bash) |

---

## ğŸ“œ Scripts Reference

All scripts live in `scripts/` and auto-detect the project root + activate the virtual environment:

| Script | Purpose |
|---|---|
| `installation.sh` | Creates venv, installs Python deps + npm packages, sets up data dirs |
| `cleanup.sh` | Resets demo state by wiping Silver, Gold, and Analytics layers |
| `generation.sh` | Generates synthetic POS, Web Order, and Warehouse data |
| `ingestion.sh` | Ingests raw CSV/JSON into Bronze layer Parquet files |
| `transform.sh` | Runs Bronze â†’ Silver â†’ Gold transformations (incl. SCD2) |
| `quality_checks.sh` | Runs 7 data quality checks, generates report |
| `kpi_analysis.sh` | Executes all 4 KPI analytics scripts, outputs JSON |
| `forecast.sh` | Trains LSTM AI brain and generates demand forecasts |
| `api.sh` | Starts the FastAPI server on port 8000 |
| `dashboard.sh` | Starts the Next.js dev server on port 3000 |

---

## ğŸ“„ Documentation

Detailed docs live in the [`docs/`](docs/) directory:

| Document | Description |
|---|---|
| [architecture.md](docs/architecture.md) | Medallion Architecture deep-dive, design decisions, trade-offs |
| [architecture_diagram.png](docs/architecture_diagram.png) | Visual system architecture diagram |
| [data_quality.md](docs/data_quality.md) | Full DQ rule catalog, thresholds, and sample evidence |
| [storage_security_plan.md](docs/storage_security_plan.md) | Parquet partitioning strategy, RBAC, encryption, audit logging |

---

## ğŸ¯ Demo Flow (for Judges)

1. **Show empty `data/` folders** â€” *"We start from zero"*
2. **Run the pipeline** (Steps 1â€“5 in Terminal 1) â€” narrate each medallion layer
3. **Start API** (Terminal 2) â€” show Swagger docs at `/docs`
4. **Start Dashboard** (Terminal 3) â€” walk through all 7 pages
5. **Highlight the journey**: Raw CSV â†’ Parquet â†’ Star Schema â†’ KPIs â†’ REST API â†’ Dashboard

---

## ğŸ¤ Contributing

1. Fork the repo
2. Create a feature branch (`git checkout -b feature/amazing-feature`)
3. Commit your changes (`git commit -m 'Add amazing feature'`)
4. Push to the branch (`git push origin feature/amazing-feature`)
5. Open a Pull Request

---

## ğŸ“œ License

This project is built for a hackathon and is open for educational use.

---

<p align="center">
  Built with â¤ï¸ by <strong>Team SixSevenCoders</strong>
</p>
