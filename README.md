<p align="center">
  <h1 align="center">Retail Data Hub</h1>
  <p align="center">
    <strong>Smart Retail Supply Chain & Customer Intelligence Platform</strong>
  </p>
  <p align="center">
    A full-stack data engineering project demonstrating ETL pipelines, Medallion Architecture, star schema modeling, automated data quality, KPI analytics, and interactive dashboarding ‚Äî all at zero infrastructure cost.
  </p>
</p>

---

## ‚ú® Highlights

| Capability | Implementation |
|---|---|
| **Architecture** | Medallion (Raw ‚Üí Bronze ‚Üí Silver ‚Üí Gold) with Parquet storage |
| **Data Modeling** | Star schema with fact & dimension tables, SCD Type 2 |
| **Ingestion** | Batch (CSV) + near real-time (JSON) with schema validation & retry |
| **Data Quality** | 7 automated checks with JSON evidence reports |
| **Analytics** | Commercial, Operations, Customer KPIs + Market Basket (Apriori) |
| **Dashboard** | 7-tab interactive Streamlit app with Plotly charts |
| **Query Engine** | DuckDB ‚Äî in-process SQL directly on Parquet files |
| **Cost** | $0 ‚Äî Python + DuckDB + Streamlit Community Cloud |

---

## üèóÔ∏è Architecture

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                        DATA SOURCES                             ‚îÇ
‚îÇ  POS Sales (CSV)  ¬∑  Web Orders (JSON)  ¬∑  Warehouse (CSV)     ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
               ‚îÇ              ‚îÇ              ‚îÇ
               ‚ñº              ‚ñº              ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  ü•â BRONZE ‚Äî Raw Parquet (schema-validated, append-only)        ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                               ‚îÇ  clean ¬∑ deduplicate ¬∑ merge
                               ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  ü•à SILVER ‚Äî Cleaned & unified sales data                       ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                               ‚îÇ  star schema ¬∑ SCD2 ¬∑ surrogate keys
                               ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  ü•á GOLD ‚Äî Star Schema (facts + dimensions, partitioned)        ‚îÇ
‚îÇ  dim_date ¬∑ dim_product ¬∑ dim_store ¬∑ dim_customer ¬∑ fact_sales ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                               ‚îÇ
               ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
               ‚ñº               ‚ñº               ‚ñº
         üìä Dashboard    üìù SQL Queries   üìà KPI Scripts
         (Streamlit)      (DuckDB)        (Python)
```

---

## üìÇ Project Structure

```
retail-data-hub/
‚îÇ
‚îú‚îÄ‚îÄ data/                          # All data layers (gitignored at scale)
‚îÇ   ‚îú‚îÄ‚îÄ raw/                       # Source files (CSV, JSON)
‚îÇ   ‚îú‚îÄ‚îÄ bronze/                    # Ingested Parquet (schema-validated)
‚îÇ   ‚îú‚îÄ‚îÄ silver/                    # Cleaned & merged Parquet
‚îÇ   ‚îî‚îÄ‚îÄ gold/                      # Star schema Parquet (partitioned)
‚îÇ
‚îú‚îÄ‚îÄ src/                           # All pipeline & analytics code
‚îÇ   ‚îú‚îÄ‚îÄ data_generation/           # Synthetic data generators (Faker + UCI)
‚îÇ   ‚îú‚îÄ‚îÄ ingestion/                 # Raw ‚Üí Bronze (batch + streaming)
‚îÇ   ‚îú‚îÄ‚îÄ transformation/            # Bronze ‚Üí Silver ‚Üí Gold (+ SCD2)
‚îÇ   ‚îú‚îÄ‚îÄ quality/                   # Automated DQ checks & JSON reports
‚îÇ   ‚îî‚îÄ‚îÄ analytics/                 # KPI scripts (commercial, ops, customer, ML)
‚îÇ
‚îú‚îÄ‚îÄ sql/                           # Standalone SQL queries for all KPIs
‚îú‚îÄ‚îÄ dashboard/                     # Streamlit app (7 interactive tabs)
‚îú‚îÄ‚îÄ docs/                          # Architecture, security & DQ documentation
‚îî‚îÄ‚îÄ README.md                      # ‚Üê You are here
```

---

## üöÄ Quick Start

### Prerequisites

- Python 3.9+
- pip

### 1. Clone & Install

```bash
git clone https://github.com/ayushmaninbox/retail-data-hub.git
cd retail-data-hub
pip install -r requirements.txt
```

### 2. Generate Synthetic Data

```bash
python src/data_generation/generate_pos.py
python src/data_generation/generate_web_orders.py
python src/data_generation/generate_warehouse.py
```

### 3. Run the Pipeline

```bash
# Ingest into Bronze
python src/ingestion/ingest_batch.py
python src/ingestion/ingest_realtime.py

# Transform Bronze ‚Üí Silver ‚Üí Gold
python src/transformation/bronze_to_silver.py
python src/transformation/silver_to_gold.py

# Run data quality checks
python src/quality/quality_checks.py
```

### 4. Launch the Dashboard

```bash
streamlit run dashboard/app.py
```

---

## üìä Dashboard Tabs

| # | Tab | What You'll See |
|---|---|---|
| 1 | üè† **Overview** | Revenue, orders, customer KPI cards + revenue trend line |
| 2 | üìä **Sales Analytics** | City-wise sales, top products, channel mix, daily trends |
| 3 | üì¶ **Inventory Health** | Stockout alerts, turnover ratio, reorder recommendations |
| 4 | üöö **Logistics** | Avg delivery time, delay distribution, bottleneck routes |
| 5 | üë• **Customers** | New vs returning, CLV distribution, RFM segments |
| 6 | üõí **Market Basket** | Item associations, confidence scores, recommendation pairs |
| 7 | ‚úÖ **Data Quality** | Pipeline health, row counts, check pass/fail status |

---

## üîë Key KPIs

### üìà Commercial
- Daily / monthly revenue aggregation
- City-wise sales breakdown (10 Indian cities)
- Top 10 products by quantity sold
- Channel mix ‚Äî POS vs Web revenue split

### üì¶ Operations
- Inventory turnover ratio per product/store
- Average delivery time per route
- Stockout rate (% of products with zero stock)
- Seasonal demand trends (monthly quantity by category)

### üë• Customer
- New vs returning customer counts
- Customer Lifetime Value (CLV)
- RFM segmentation (Recency ¬∑ Frequency ¬∑ Monetary)

### üõí AI / ML
- Market basket analysis using **Apriori algorithm**
- Association rules with support, confidence & lift scores

---

## üõ°Ô∏è Data Quality Framework

Seven automated checks run at every pipeline stage:

| # | Check | Rule | On Failure |
|---|---|---|---|
| 1 | No negative prices | `unit_price >= 0` | Quarantine row |
| 2 | No future dates | `date <= today()` | Reject row |
| 3 | No null customer IDs | `customer_id IS NOT NULL` | Fill "UNKNOWN" |
| 4 | No duplicates | Composite key uniqueness | Drop duplicate |
| 5 | Referential integrity | FK exists in dimension | Reject orphan |
| 6 | Quantity range | `1 <= qty <= 10,000` | Flag outlier |
| 7 | Column completeness | `% non-null per column` | Report metric |

Results are saved as `data_quality_report.json` and visualized in the dashboard.

---

## üß∞ Tech Stack

| Layer | Technology |
|---|---|
| Language | Python 3.9+ |
| Data Generation | Faker, Pandas, UCI Online Retail Dataset |
| Storage Format | Apache Parquet (columnar, compressed) |
| Query Engine | DuckDB (in-process OLAP) |
| Transformations | Pandas, DuckDB SQL |
| ML / Analytics | mlxtend (Apriori), Pandas |
| Dashboard | Streamlit + Plotly |
| Deployment | Streamlit Community Cloud (free) |

---

## üìÑ Documentation

Detailed docs live in the [`docs/`](docs/) directory:

- **Storage & Security Plan** ‚Äî Parquet partitioning strategy, RBAC, encryption, audit logging
- **Data Quality Documentation** ‚Äî Full DQ rule catalog, thresholds, and sample evidence
- **Architecture Notes** ‚Äî Medallion Architecture design decisions and trade-offs

---

## ü§ù Contributing

1. Fork the repo
2. Create a feature branch (`git checkout -b feature/amazing-feature`)
3. Commit your changes (`git commit -m 'Add amazing feature'`)
4. Push to the branch (`git push origin feature/amazing-feature`)
5. Open a Pull Request

---

## üìú License

This project is built for a hackathon and is open for educational use.

---

<p align="center">
  Built with ‚ù§Ô∏è by Team SixSevenCoders
</p>
